{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# open data\n",
    "# Fix beam search algorithm\n",
    "# run beam search\n",
    "# run results code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import sys\n",
    "# import logging\n",
    "# import datetime\n",
    "import dill as pickle\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # original py files\n",
    "# sys.path.insert(1, 'methods')\n",
    "\n",
    "from data_methods import getData, standardize\n",
    "from dimensionality_reduction import reduce_dimensionality,reduce_with\n",
    "from beamSearch import EMM, as_string\n",
    "from adjPysubgroup import adjustedBestFirstSearch, adjustedDFS, adjustedApriori\n",
    "from qualityMeasures import calc_result_bs, calc_result_ps\n",
    "from interpretabilityMeasures import Feature_Correlation_Scores, DBI_beam, DBI_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get all datasets\n",
    "# lst_dataset_names = [\"Ionosphere\",\"Mushroom\",\"Adult\",\"Soybean\",\"Arrhythmia\", \"Indoor\"] #,\"Indoor\" not in data folder\n",
    "# path = r\"W:\\OneDrive - TU Eindhoven\\DS&AI\\2024-2025\\2024-2025 q1\\2AMM20 - Research Topics in Data Mining\\Research Project Phase\\GitHub Code\\Interpretable-Subgroup-Discovery-1\\data\"\n",
    "# dct_datasets = {name:{key:val for val,key in zip(getData(name,path),['df','cat','num','features'])} for name in lst_dataset_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_datasets = {}\n",
    "\n",
    "for dataset in ['Ionosphere','Mushroom','Adult','Soybean','Arrhythmia','Indoor']:\n",
    "\n",
    "    with open(os.path.join(\n",
    "        r'W:\\OneDrive - TU Eindhoven\\DS&AI\\2024-2025\\2024-2025 q1\\2AMM20 - Research Topics in Data Mining\\Research Project Phase\\GitHub Code\\Interpretable-Subgroup-Discovery-1\\results_renamed',\n",
    "        f'{dataset}-data-reductions.pkl'), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    dct_datasets[dataset] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Problem 1: </b> ```eta``` function generates all possible refinements of a seed by adding new conditions based on all features, regardless of whether a feature is already used in the seed; it does not prevent the addition of new conditions on attributes that are already used in the seed. Thus, seeds with a certain attribute can be refined by adding another condition that includes the same attribute.<br>\n",
    "<b>Solution:</b> modify the ```eta``` function so that it is only possible to refine a seed using attributes that are not already present in the seed.\n",
    "\n",
    "\n",
    "<b>Problem 2:</b> The current Beam Search algorithm does not check whether subgroups with similar conditions contain the same subgroups, e.g. attr1 <= 5 AND attr2 > 3 might select the same observations as attr1 <= 5 AND attr2 >= 4. This results in duplicate subgroups, defined by fairly similar rules. <br>\n",
    "<b>Solution:</b> Check for subgroup equivalence before adding a new subgroup to the result set to verify whether it is unique in its observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# The following code was adapted from W. Duivesteijn, T.C. van Dijk. (2021)\n",
    "#     Exceptional Gestalt Mining: Combining Magic Cards to Make Complex Coalitions Thrive. \n",
    "#     In: Proceedings of the 8th Workshop on Machine Learning and Data Mining for Sports Analytics.\n",
    "#     Available from http://wwwis.win.tue.nl/~wouter/Publ/J05-EMM_DMKD.pdf\n",
    "# \"\"\"\n",
    "\n",
    "# # Package imports\n",
    "# import heapq\n",
    "# import numpy as np\n",
    "\n",
    "# # Classes\n",
    "# class BoundedPriorityQueue:\n",
    "#     \"\"\"\n",
    "#     Used to store the <q> most promising subgroups\n",
    "#     Ensures uniqness\n",
    "#     Keeps a maximum size (throws away value with least quality)\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, bound): \n",
    "#         # Initializes empty queue with maximum length of <bound>\n",
    "#         self.values = []\n",
    "#         self.bound = bound\n",
    "#         self.entry_count = 0\n",
    "\n",
    "#     def add(self, element, quality, **adds): \n",
    "#         # Adds <element> to the bounded priority queue if it is of sufficient quality\n",
    "#         new_entry = (quality, self.entry_count, element, adds)\n",
    "#         if (len(self.values) >= self.bound):\n",
    "#             heapq.heappushpop(self.values, new_entry)\n",
    "#         else:\n",
    "#             heapq.heappush(self.values, new_entry)\n",
    "\n",
    "#         self.entry_count += 1\n",
    "\n",
    "#     def get_values(self):\n",
    "#         # Returns elements in bounded priority queue in sorted order\n",
    "#         for (q, _, e, x) in sorted(self.values, reverse=True):\n",
    "#             yield (q, e, x)\n",
    "\n",
    "#     def show_contents(self):  \n",
    "#         # Prints contents of the bounded priority queue (used for debugging)\n",
    "#         print(\"show_contents\")\n",
    "#         for (q, entry_count, e) in self.values:\n",
    "#             print(q, entry_count, e)\n",
    "\n",
    "# class Queue:\n",
    "#     \"\"\"\n",
    "#     Used to store candidate solutions\n",
    "#     Ensures uniqness\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self): # Initializes empty queue\n",
    "#         self.items = []\n",
    "\n",
    "#     def is_empty(self): # Returns True if queue is empty, False otherwise\n",
    "#         return self.items == []\n",
    "\n",
    "#     def enqueue(self, item): # Adds <item> to queue if it is not already present\n",
    "#         if item not in self.items:\n",
    "#             self.items.insert(0, item)\n",
    "\n",
    "#     def dequeue(self): # Pulls one item from the queue\n",
    "#         return self.items.pop()\n",
    "\n",
    "#     def size(self): # Returns the number of items in the queue\n",
    "#         return len(self.items)\n",
    "\n",
    "#     def get_values(self): # Returns the queue (as a list)\n",
    "#         return self.items\n",
    "\n",
    "#     def add_all(self, iterable): # Adds all items in <iterable> to the queue, given they are not already present\n",
    "#         for item in iterable:\n",
    "#             self.enqueue(item)\n",
    "\n",
    "#     def clear(self): # Removes all items from the queue\n",
    "#         self.items.clear()\n",
    "        \n",
    "# # Functions\n",
    "# def refine(desc, more):\n",
    "#     # Creates a copy of the seed <desc> and adds it to the new selector <more>\n",
    "#     # Used to prevent pointer issues with selectors\n",
    "#     copy = desc[:]\n",
    "#     copy.append(more)\n",
    "#     return copy\n",
    "\n",
    "# def as_string(desc):\n",
    "#     # Adds ' and ' to <desc> such that selectors are properly separated when the refine function is used\n",
    "#     return ' and '.join(desc)\n",
    "\n",
    "# def eta(seed, df, features, n_chunks = 5,prnt=False):\n",
    "#     # Returns a generator which includes all possible refinements of <seed> for the given <features> on dataset <df>\n",
    "#     # n_chunks refers to the number of possible splits we consider for numerical features\n",
    "#     if prnt:\n",
    "#         print(\"eta \", seed)\n",
    "\n",
    "#     # ! Find features present in the current seed and exclude them from the possible features:\n",
    "#     # ! -> function currently only checks whether the condition is already in there, allowing for different conditions on the same attributes\n",
    "#     used_features = set()\n",
    "#     for condition in seed:\n",
    "#         feature_name = condition.split()[0]\n",
    "#         used_features.add(feature_name)\n",
    "#     available_features = [f for f in features if f not in used_features]\n",
    "#     # ! end of adjusted code\n",
    "\n",
    "#     # only specify more on the elements that are still in the subset\n",
    "#     if seed != []:              \n",
    "#         d_str = as_string(seed)\n",
    "#         ind = df.eval(d_str)\n",
    "#         df_sub = df.loc[ind, ]\n",
    "#     else:\n",
    "#         df_sub = df\n",
    "\n",
    "#     for f in available_features:\n",
    "#         # if (df_sub[f].dtype == 'float64') or (df_sub[f].dtype == 'float32'): #get quantiles here instead of intervals for the case that data are very skewed\n",
    "#         if pd.api.types.is_numeric_dtype(df_sub[f]): # ! more reliable than comparing it to a float as .dtype returns an object and not a string\n",
    "#             column_data = df_sub[f]\n",
    "#             dat = np.sort(column_data)\n",
    "#             dat = dat[np.logical_not(np.isnan(dat))]\n",
    "#             for i in range(1,n_chunks+1): #determine the number of chunks you want to divide your data in\n",
    "#                 x = np.percentile(dat, 100 * i / n_chunks) # ! changed from 100 / i which is INCORRECT\n",
    "#                 candidate_leq = f\"{f} <= {x}\" # ! changed from .format to fstring\n",
    "#                 # if not candidate in seed: # if not already there\n",
    "#                 yield refine(seed, candidate_leq)\n",
    "#                 candidate_gt = f\"{f} > {x}\"\n",
    "#                 # if not candidate in seed: # if not already there\n",
    "#                 yield refine(seed, candidate_gt)\n",
    "#         # elif (df_sub[f].dtype == 'object'):\n",
    "#         if pd.api.types.is_categorical_dtype(df_sub[f]) or pd.api.types.is_object_dtype(df_sub[f]): # ! more reliable\n",
    "#             column_data = df_sub[f]\n",
    "#             uniq = column_data.dropna().unique()\n",
    "#             for i in uniq:\n",
    "#                 candidate_eq = f\"{f} == '{i}'\"\n",
    "#                 # if not candidate in seed: # if not already there\n",
    "#                 yield refine(seed, candidate_eq)\n",
    "#                 candidate_neq = f\"{f} != '{i}'\"\n",
    "#                 # if not candidate in seed: # if not already there\n",
    "#                 yield refine(seed, candidate_neq)\n",
    "#         # elif (df_sub[f].dtype == 'int64'):\n",
    "#         #     column_data = df_sub[f]\n",
    "#         #     dat = np.sort(column_data)\n",
    "#         #     dat = dat[np.logical_not(np.isnan(dat))]\n",
    "#         #     for i in range(1,n_chunks+1): #determine the number of chunks you want to divide your data in\n",
    "#         #         x = np.percentile(dat,100/i) #\n",
    "#         #         candidate = \"{} <= {}\".format(f, x)\n",
    "#         #         # if not candidate in seed: # if not already there\n",
    "#         #         yield refine(seed, candidate)\n",
    "#         #         candidate = \"{} > {}\".format(f, x)\n",
    "#         #         # if not candidate in seed: # if not already there\n",
    "#         #         yield refine(seed, candidate)\n",
    "#         # elif (df_sub[f].dtype == 'bool'):\n",
    "#         elif pd.api.types.is_bool_dtype(df_sub[f]): # ! more reliable\n",
    "#             # uniq = column_data.dropna().unique()\n",
    "#             for i in [True,False]: # ! instead of unique\n",
    "#                 candidate_eq = f\"{f} == '{i}'\"\n",
    "#                 # if not candidate in seed: # if not already there\n",
    "#                 yield refine(seed, candidate_eq)\n",
    "#                 candidate_neq = f\"{f} != '{i}'\"\n",
    "#                 # if not candidate in seed: # if not already there\n",
    "#                 yield refine(seed, candidate_neq)\n",
    "#         else:\n",
    "#             # assert False\n",
    "#             continue # ! skip for unsupported dtypes\n",
    "\n",
    "# def satisfies_all(desc, df, threshold=0.02):\n",
    "#     # Function used to check if subgroup with pattern <desc> is sufficiently big relative to its dataset <df>\n",
    "#     # A subgroup is sufficiently big if the proportion of data included in it exceeds <threshold>   \n",
    "#     d_str = as_string(desc)\n",
    "#     ind = df.eval(d_str)\n",
    "#     return sum(ind) >= len(df) * 0.02 \n",
    "\n",
    "# def eval_quality(desc, df, target):\n",
    "#     # Function used to calculate the solution's WRAcc\n",
    "#     sub_group = df[df.eval(as_string(desc))] \n",
    "#     prop_p_sg = len(sub_group[sub_group[target]==1])/len(sub_group)\n",
    "#     prop_p_df = len(df[df[target]==1])/len(df)\n",
    "#     wracc = ((len(sub_group)/len(df))**1) * (prop_p_sg - prop_p_df) #for WRAcc a=1\n",
    "#     return wracc\n",
    "\n",
    "\n",
    "\n",
    "# def EMM(w, d, q, catch_all_description, df, target, n_chunks=5, ensure_diversity = False,prnt_level=True,prnt_seed=True,prnt_eta=False):\n",
    "#     \"\"\"\n",
    "#     w - width of beam, i.e. the max number of results in the beam\n",
    "#     d - num levels, i.e. how many attributes are considered\n",
    "#     q - max results, i.e. max number of results output by the algorithm\n",
    "#     eta - a function that receives a description and returns all possible refinements\n",
    "#     satisfies_all - a function that receives a description and verifies wheather it satisfies some requirements as needed\n",
    "#     eval_quality - returns a quality for a given description. This should be comparable to qualities of other descriptions\n",
    "#     catch_all_description - the equivalent of True, or all, as that the whole dataset shall match\n",
    "#     df - dataframe of mined dataset\n",
    "#     features - features in scope\n",
    "#     target - column name of target attribute in df\n",
    "#     \"\"\"\n",
    "#     features = [col for col in df.columns if col!='target']\n",
    "    \n",
    "#     # Initialize variables\n",
    "#     resultSet = BoundedPriorityQueue(q) # Set of results, can contain results from multiple levels\n",
    "#     candidateQueue = Queue() # Set of candidate solutions to consider adding to the ResultSet\n",
    "#     candidateQueue.enqueue(catch_all_description) # Set of results on a particular level\n",
    "#     error = 0.00001 # Allowed error margin (due to floating point error) when comparing the quality of solutions\n",
    "\n",
    "#     # ! keep track of seen_subgroups:\n",
    "#     seen_subgroups = set()\n",
    "\n",
    "#     # Perform BeamSearch for <d> levels\n",
    "#     for level in range(d):\n",
    "#         if prnt_level:\n",
    "#             print(\"level : \", level)\n",
    "        \n",
    "#         # Initialize this level's beam\n",
    "#         beam = BoundedPriorityQueue(w)\n",
    "\n",
    "#         # Go over all rules generated on previous level, or 'empty' rule if level = 0 \n",
    "#         for seed in candidateQueue.get_values():\n",
    "#             if prnt_seed:\n",
    "#                 print(\"    seed : \", seed)\n",
    "            \n",
    "#             # Start by evaluating the quality of the seed\n",
    "#             if seed != []:\n",
    "#                 seed_quality = eval_quality(seed, df, target)\n",
    "#                 beam.add(seed, seed_quality) # ! include seed itself in the beam\n",
    "#             else:\n",
    "#                 seed_quality = 99\n",
    "\n",
    "#             # For all refinements created by eta function on descriptions (i.e features), which can be different types of columns\n",
    "#             # eta(seed) reads the dataset given certain seed (i.e. already created rules) and looks at new descriptions\n",
    "#             for desc in eta(seed, df, features, n_chunks,prnt=prnt_eta):\n",
    "\n",
    "#                 # Check if the subgroup contains at least x% of data, proceed if yes\n",
    "#                 if satisfies_all(desc, df):\n",
    "#                     # generate hash of subgroup indices\n",
    "#                     subgroup_indices = df[df.eval(as_string(desc))].index\n",
    "#                     subgroup_hash = tuple(sorted(subgroup_indices))\n",
    "\n",
    "#                     # skip if the (hash of the) subgroup has already been visited\n",
    "#                     if subgroup_hash in seen_subgroups:\n",
    "#                         continue\n",
    "#                     # if it has not been seen, we continue and add the new hash to the set\n",
    "#                     seen_subgroups.add(subgroup_hash)\n",
    "\n",
    "#                     # Calculate the new solution's quality\n",
    "#                     quality = eval_quality(desc, df, target)\n",
    "                    \n",
    "#                     # Ensure diversity by forcing difference in quality when compared to its seed\n",
    "#                     # if <ensure_diversity> is set to True. Principle is based on:\n",
    "#                     # Van Leeuwen, M., & Knobbe, A. (2012), Diverse subgroup set discovery.\n",
    "#                     # Data Mining and Knowledge Discovery, 25(2), 208-242.\n",
    "#                     if ensure_diversity:\n",
    "#                         # if quality < (seed_quality * 1-error) or quality > (seed_quality * 1+error) : # ! <- irrelevantly long condition\n",
    "#                         if abs(quality - quality) > error:\n",
    "#                             resultSet.add(desc, quality)\n",
    "#                             beam.add(desc, quality)\n",
    "#                     else:\n",
    "#                         resultSet.add(desc, quality)\n",
    "#                         beam.add(desc, quality)\n",
    "\n",
    "#         # When all candidates for a search level have been explored, \n",
    "#         # the contents of the beam are moved into candidateQueue, to generate next level candidates\n",
    "#         candidateQueue = Queue()\n",
    "#         candidateQueue.add_all(desc for (_, desc, _) in beam.get_values())\n",
    "        \n",
    "#     # Return the <resultSet> once the BeamSearch algorithm has completed\n",
    "#     return resultSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Beam Search algo's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level :  0\n",
      "    seed :  []\n",
      "eta  []\n",
      "level :  1\n",
      "    seed :  [\"attribute17 == '1'\"]\n",
      "eta  [\"attribute17 == '1'\"]\n",
      "    seed :  [\"attribute1 == '4'\"]\n",
      "eta  [\"attribute1 == '4'\"]\n",
      "    seed :  [\"attribute1 != '1'\"]\n",
      "eta  [\"attribute1 != '1'\"]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m dct_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSoybean\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvanilla\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m \u001b[43mEMM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_diversity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprnt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mw:\\OneDrive - TU Eindhoven\\DS&AI\\2024-2025\\2024-2025 q1\\2AMM20 - Research Topics in Data Mining\\Research Project Phase\\GitHub Code\\Interpretable-Subgroup-Discovery-1\\methods\\beamSearch.py:209\u001b[0m, in \u001b[0;36mEMM\u001b[1;34m(w, d, q, catch_all_description, df, target, n_chunks, ensure_diversity, prnt)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# For all refinements created by eta function on descriptions (i.e features), which can be different types of columns\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# eta(seed) reads the dataset given certain seed (i.e. already created rules) and looks at new descriptions\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m desc \u001b[38;5;129;01min\u001b[39;00m eta(seed, df, features, n_chunks):\n\u001b[0;32m    207\u001b[0m \n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Check if the subgroup contains at least x% of data, proceed if yes\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msatisfies_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    210\u001b[0m \n\u001b[0;32m    211\u001b[0m         \u001b[38;5;66;03m# Calculate the new solution's quality\u001b[39;00m\n\u001b[0;32m    212\u001b[0m         quality \u001b[38;5;241m=\u001b[39m eval_quality(desc, df, target)\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;66;03m# Ensure diversity by forcing difference in quality when compared to its seed\u001b[39;00m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;66;03m# if <ensure_diversity> is set to True. Principle is based on:\u001b[39;00m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;66;03m# Van Leeuwen, M., & Knobbe, A. (2012), Diverse subgroup set discovery.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;66;03m# Data Mining and Knowledge Discovery, 25(2), 208-242.\u001b[39;00m\n",
      "File \u001b[1;32mw:\\OneDrive - TU Eindhoven\\DS&AI\\2024-2025\\2024-2025 q1\\2AMM20 - Research Topics in Data Mining\\Research Project Phase\\GitHub Code\\Interpretable-Subgroup-Discovery-1\\methods\\beamSearch.py:153\u001b[0m, in \u001b[0;36msatisfies_all\u001b[1;34m(desc, df, threshold)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msatisfies_all\u001b[39m(desc, df, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m):\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# Function used to check if subgroup with pattern <desc> is sufficiently big relative to its dataset <df>\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# A subgroup is sufficiently big if the proportion of data included in it exceeds <threshold>   \u001b[39;00m\n\u001b[0;32m    152\u001b[0m     d_str \u001b[38;5;241m=\u001b[39m as_string(desc)\n\u001b[1;32m--> 153\u001b[0m     ind \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(ind) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.02\u001b[39m\n",
      "File \u001b[1;32mw:\\Users\\wfeij\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\frame.py:4943\u001b[0m, in \u001b[0;36mDataFrame.eval\u001b[1;34m(self, expr, inplace, **kwargs)\u001b[0m\n\u001b[0;32m   4941\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4942\u001b[0m index_resolvers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_index_resolvers()\n\u001b[1;32m-> 4943\u001b[0m column_resolvers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cleaned_column_resolvers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4944\u001b[0m resolvers \u001b[38;5;241m=\u001b[39m column_resolvers, index_resolvers\n\u001b[0;32m   4945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32mw:\\Users\\wfeij\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\generic.py:659\u001b[0m, in \u001b[0;36mNDFrame._get_cleaned_column_resolvers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCSeries):\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {clean_column_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname): \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m--> 659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    660\u001b[0m     clean_column_name(k): Series(\n\u001b[0;32m    661\u001b[0m         v, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39mk, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtypes[k]\n\u001b[0;32m    662\u001b[0m     )\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_column_arrays())\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    665\u001b[0m }\n",
      "File \u001b[1;32mw:\\Users\\wfeij\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\generic.py:661\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCSeries):\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {clean_column_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname): \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    660\u001b[0m     clean_column_name(k): Series(\n\u001b[1;32m--> 661\u001b[0m         v, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39mk, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m[k]\n\u001b[0;32m    662\u001b[0m     )\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_column_arrays())\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    665\u001b[0m }\n",
      "File \u001b[1;32mw:\\Users\\wfeij\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\generic.py:6461\u001b[0m, in \u001b[0;36mNDFrame.dtypes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   6434\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6435\u001b[0m \u001b[38;5;124;03mReturn the dtypes in the DataFrame.\u001b[39;00m\n\u001b[0;32m   6436\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6458\u001b[0m \u001b[38;5;124;03mdtype: object\u001b[39;00m\n\u001b[0;32m   6459\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6460\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mget_dtypes()\n\u001b[1;32m-> 6461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_sliced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mw:\\Users\\wfeij\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\series.py:574\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m [data]\n\u001b[0;32m    573\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_list_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    575\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(data, index)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;66;03m# create/copy the manager\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = dct_datasets['Soybean']['vanilla']\n",
    "EMM(100, 3, 100, [], df, 'target', ensure_diversity=True, prnt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attribute1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"attribute1 != '1'\".split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"attribute1 != '1'\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
